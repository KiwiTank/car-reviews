{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# Assignment 2 - Car Reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "## Data preprocessing\n",
    "\n",
    "In order for the two algorithms used in this assignment to function efficiently, the data needed to be processesed in order to remove redundant punctuation, characters, and words while also converting all the reviews into lower case. Further to this, each word was stemmed using NLTK's PorterStemmer function so that different words derived from the same root will be counted as a single occurance of the same word. \n",
    "\n",
    "#### Redundant chracter/word removal\n",
    "\n",
    "The raw data (*car-reviews.csv*) was imported and converted to a dataframe, dropping any duplicates. All further processing was done in a loop on each review in turn:\n",
    "- Each review was first converted to lower case;\n",
    "- The review was then tokenised, in order to process each word one by one, using the wordpunkt tokeniser which would create tokens of all punctation seperately from words;\n",
    "- Each word was then added to a list if it was not a stopword (using NLTK's stopword list), did not contain numbers, and was not punctuation. Words that did not fulfill these criteria were dropped.\n",
    "\n",
    "#### Stemming and exporting\n",
    "\n",
    "The cleaned word list created for a review string in the above loop was then stemmed using the PorterStemmer function. This list of stems was recombined into a single string, with each word seperated by a blank space in order to patch the review back together. A dictionary pairing each review with its matching sentiment was created and then returned as a pandas dataframe.\n",
    "\n",
    "The code to run this data preprocessing is shown in the two cells below and needs to be run in order to create the dataframe needed for the assignment's two tasks (runtime ~1 min):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules used for preprocessing\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import sys\n",
    "\n",
    "# function to identify words that contain numbers\n",
    "def contains_nums(text):\n",
    "    if any(char.isdigit() for char in text):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# function to preprocess the car reviews strings, as discussed in the Markdown cell above, and return\n",
    "# the input csv as a processed pandas dataframe\n",
    "def preprocess(file):\n",
    "    df1 = pd.read_csv(file)\n",
    "\n",
    "    df1 = df1.drop_duplicates()\n",
    "\n",
    "    length_of_df = df1.shape[0] - 1\n",
    "    count = 0\n",
    "    all_comments = []\n",
    "    all_sent = df1['Sentiment']\n",
    "\n",
    "    sw = list(stopwords.words())\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for rev in df1['Review']:\n",
    "        rev = rev.lower()\n",
    "        rev_tokens = wordpunct_tokenize(rev)\n",
    "        rev_clean = [word for word in rev_tokens if word not in sw and not \n",
    "                       contains_nums(word) and word not in string.punctuation]\n",
    "        stemmed = [stemmer.stem(word) for word in rev_clean]\n",
    "        rev_combined = \" \".join(stemmed)\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(f'Processing {file}... {round(count/length_of_df*100, 0)}%')\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        all_comments.append(rev_combined)\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(f'Processing of {file} complete')\n",
    "    \n",
    "    d = {'Sentiment': all_sent, 'Reviews': all_comments}\n",
    "\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing of car-reviews.csv complete"
     ]
    }
   ],
   "source": [
    "# imports the raw data csv as a dataframe, then runs the preprocessing function to create the \n",
    "# processed dataframe to be used in the two tasks\n",
    "df_raw = pd.read_csv('car-reviews.csv')\n",
    "df_processed = preprocess('car-reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stemming examples\n",
    "\n",
    "The function in the cell below shows three examples of where stemming has taken places between the raw and processed data. Given the root of a word found in multiple places in the review strings, it shows that while the root occurs within multiple variants of words in the raw data, these have all been converted to a common stem in the processed data.\n",
    "\n",
    "The three stems shown below are *'purchas'*, *'wonder'*, and *'decid'*. The function's results clearly shows that the raw data contains multiple different words that share the same stem, while the processed data has replaced all occurences of these words with the stem itself.\n",
    "\n",
    "*NB: This function requires access to both the raw and processed dataframes created in the cell above*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compare the presence of words from the raw and processed car review data, and \n",
    "# return all unique words in each dataframe containing that stem\n",
    "def compare_stems(stem, raw, proc, row_count=100):\n",
    "    df_find = raw.copy()\n",
    "    stem_list_raw = []\n",
    "    for row in df_find['Review'][:row_count]:\n",
    "        words_list = row.split()\n",
    "        for word in words_list:\n",
    "            if stem in word:\n",
    "                stem_list_raw.append(word)\n",
    "\n",
    "    df_find_stem = proc.copy()\n",
    "    stem_list = []\n",
    "    for row in df_find_stem['Reviews'][:row_count]:\n",
    "        words_list2 = row.split()\n",
    "        for word in words_list2:\n",
    "            if stem in word:\n",
    "                stem_list.append(word)\n",
    "\n",
    "    raw_set = set(stem_list_raw)\n",
    "    proc_set = set(stem_list)\n",
    "\n",
    "    return print(\n",
    "        f'Unique occurrences of words containing \\'{stem}\\' in the first {row_count} rows '\n",
    "        f'for -\\nRaw data: {raw_set} \\nProcessed data: {proc_set}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique occurrences of words containing 'purchas' in the first 100 rows for -\n",
      "Raw data: {'purchasing', 'purchase', 'purchased'} \n",
      "Processed data: {'purchas'}\n"
     ]
    }
   ],
   "source": [
    "compare_stems('purchas', df_raw, df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique occurrences of words containing 'wonder' in the first 100 rows for -\n",
      "Raw data: {'wondering', 'wonder', 'wonderful'} \n",
      "Processed data: {'wonder'}\n"
     ]
    }
   ],
   "source": [
    "compare_stems('wonder', df_raw, df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique occurrences of words containing 'decid' in the first 100 rows for -\n",
      "Raw data: {'decides', 'decide', 'decided'} \n",
      "Processed data: {'decid'}\n"
     ]
    }
   ],
   "source": [
    "compare_stems('decid', df_raw, df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 1 - Naive Bayes\n",
    "\n",
    "#### Code variables & output\n",
    "\n",
    "The function defined below (*nb_sent_class*) runs a multinomial naive bayes algorithm on an input dataframe of preprocessed data and returns three things:\n",
    "- A six-row dataframe illustrating the vectors created for each car review string;\n",
    "- The predicted accuracy of the algorithm on the testing data alongside a more detailed table of results;\n",
    "- Lastly, a confusion matrix plot showing the number of correct and incorrect predictions made on the testing data.\n",
    "\n",
    "A multinomial model was chosen as it handles text classification well, while also being able to add smoothing to handle any words not seen in the training dataset. A complement naive bayes model was also possible, however that is better suited to poorly balanced datasets by using the complement probability to assign the class, which would not be ideally suited to the well-balanced dataset found in *car-reviews.csv*. However, given that this is not a significantly large dataset, these two algorithms will return very similar results and could have been used interchangebly. \n",
    "\n",
    "For the implementation of the multinomial algorithm itself, it was decided to keep the parameters entirely as the defaults in order to provide a good base case for Task 2 to improve upon. Therefore, the function only selects unigrams of words to form the vector transformation. \n",
    "\n",
    "#### Training and testing data split\n",
    "\n",
    "In order to create the testing and training data, and particularly to avoid any data leakage, sklearn's *train_test_split* function was used. This function both shuffles and splits the input array into defined training and testing sets. For this function I defined the *test_size* to be 20% of the complete dataset, leaving the remaining 80% as the training set. Leaving the *shuffle* as the default *True* meant the data was shuffled before the split. To complement this the *stratify* variable was assigned to the *Sentiment* class labels of the data, which ensures that an even distribution of **Pos** and **Neg** class labels end up in both training and testing datasets. Lastly, I set the *random_state* to an arbitrary 99 value to allow for comparison between different runs and between tasks 1 and 2. \n",
    "\n",
    "#### Unseen words in testing or training set\n",
    "\n",
    "An important variable in the multinomial naive bayes algorithm is *alpha=1*, which is the default value and is used in this task. By keeping alpha > 0 we can remove the possibility of a zero probability when a word is not seen in the training set but is present in the testing set. Were alpha to be zero, it is possible to have a divide by zero error and the algorithm would fail. \n",
    "\n",
    "This works by adding 1 to each count of word appearances in the numerator, and then also adding the 1 to the total count of all features in the denominator. This particular implmentation is called Laplace smoothing, and is applied here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports the modules needed for the two algorithms used in tasks one and two\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_sent_class(df_in, test_perc=0.2, rand=99):\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # create the vectoriser object\n",
    "    vec = CountVectorizer()\n",
    "\n",
    "    x = df['Reviews']\n",
    "    y = df['Sentiment']\n",
    "    \n",
    "    classes = ['Pos', 'Neg']\n",
    "    \n",
    "    # split the data into training and testing sets (80:20), stratified on the class to ensure an even\n",
    "    # spread of the two sentiment types\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=test_perc, \n",
    "                                                        random_state=rand)\n",
    "    \n",
    "    # transform the data into a numerical array\n",
    "    x_train = vec.fit_transform(x_train).toarray()\n",
    "    x_test = vec.transform(x_test).toarray()\n",
    "    \n",
    "    # get the word/stem labels for each column in the array and return a dataframe of the first\n",
    "    # six rows to illustrate the matrix format\n",
    "    labels = vec.get_feature_names_out()\n",
    "    rows = y[:6]\n",
    "    data = x_train[:6]\n",
    "    vect_df = pd.DataFrame(data=data, index=rows, columns=labels)\n",
    "    print('Dataframe showing first 6 rows, illustrating the matrix of stem counts created for each review')\n",
    "    display(vect_df)\n",
    "    \n",
    "    # create the multinomial naive bayes model and train with the training data\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # create a prediction on the test data and return the accuracy of the prediction, along with a \n",
    "    # confusion matrix plot of the results\n",
    "    prediction = model.predict(x_test)\n",
    "    accuracy = metrics.accuracy_score(prediction, y_test)\n",
    "    print('\\n')\n",
    "    print(str('Predicted accuracy for Naive Bayes is {:04.2f}'.format(accuracy * 100)) + '%')\n",
    "    print('\\n')\n",
    "    print(classification_report(y_test, prediction,target_names=classes))\n",
    "    con_mat = confusion_matrix(y_test, prediction)\n",
    "    figure = ConfusionMatrixDisplay(confusion_matrix=con_mat, display_labels=model.classes_)\n",
    "    figure = figure.plot(cmap=plt.cm.plasma)\n",
    "    figure.ax_.set(title='Naive Bayes Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe showing first 6 rows, illustrating the matrix of stem counts created for each review\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaf</th>\n",
       "      <th>aah</th>\n",
       "      <th>aamco</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abcnew</th>\n",
       "      <th>abil</th>\n",
       "      <th>abism</th>\n",
       "      <th>abnorm</th>\n",
       "      <th>...</th>\n",
       "      <th>zippi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zr</th>\n",
       "      <th>zt</th>\n",
       "      <th>ztec</th>\n",
       "      <th>ztech</th>\n",
       "      <th>ztw</th>\n",
       "      <th>zx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 8548 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           aaa  aaf  aah  aamco  ab  abandon  abcnew  abil  abism  abnorm  \\\n",
       "Sentiment                                                                   \n",
       "Neg          0    0    0      0   0        0       0     0      0       0   \n",
       "Neg          0    0    0      0   0        0       0     0      0       0   \n",
       "Neg          0    0    0      0   0        0       0     0      0       0   \n",
       "Neg          0    0    0      0   0        0       0     0      0       0   \n",
       "Neg          0    0    0      0   0        0       0     0      0       0   \n",
       "Neg          0    0    0      0   0        0       0     0      0       0   \n",
       "\n",
       "           ...  zippi  zone  zoo  zoom  zr  zt  ztec  ztech  ztw  zx  \n",
       "Sentiment  ...                                                        \n",
       "Neg        ...      0     0    0     0   0   0     0      0    0   0  \n",
       "Neg        ...      0     0    0     0   0   0     0      0    0   0  \n",
       "Neg        ...      0     0    0     0   0   0     0      0    0   0  \n",
       "Neg        ...      0     0    0     0   0   0     0      0    0   0  \n",
       "Neg        ...      0     0    0     0   0   0     0      0    0   0  \n",
       "Neg        ...      0     0    0     0   0   0     0      0    0   0  \n",
       "\n",
       "[6 rows x 8548 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Predicted accuracy for Naive Bayes is 79.42%\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pos       0.79      0.80      0.79       138\n",
      "         Neg       0.80      0.79      0.79       139\n",
      "\n",
      "    accuracy                           0.79       277\n",
      "   macro avg       0.79      0.79      0.79       277\n",
      "weighted avg       0.79      0.79      0.79       277\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_sent_class(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2 - Improved solution\n",
    "\n",
    "#### Approach and expected improvement\n",
    "\n",
    "While Naive Bayes (NB) does a commendable job when classifying data (i.e. when ranking a review's class probability), it is not particularly good when used for prediction on unseen data as it tends to be overconfident when estimating a class as a result of its assumption of independance (Russel and Norvig, 2020). Furthermore, NB calculates the class prior probabilities based on the supplied training data, which may not be representative of the real world probabilities (Starmer, 2022). In the case of consumer reviews, research has shown that people are around ten times more likely to post a negative review than a positive one (Thomas, 2018), meaning that because we have supplied negative and positive reviews in equal amounts NB will not be able to accurately predict the prior probability that a given review belongs to a specific class if it were tested on more imbalanced real world data. \n",
    "\n",
    "In order to improve on the prediction accuracy of Task 1 I opted to use a linear support vector machine (SVM), while also modifying the vectorising approach to better capture differences between review strings. \n",
    "\n",
    "###### Vectoriser changes\n",
    "\n",
    "The Naive Bayes function in Task 1 used the *CountVectorizer* function to convert each review into a usable matrix of values, each corresponding to the count of a given word, and thus create a 'Bag of Words'. The problem with this approach is that, in the case of sentiment, it might be that words that appear less frequently would be overshadowed by those that are very common, while actually they should have more impact on whether a review could be seen as positive or negative than the more abundant words. \n",
    "\n",
    "To counter this I applied a *Term Frequency - Inverse Document Frequency* (Tfidf) function, which takes the counts of word frequency as in *CountVectorizer* and converts them into weights by multiplying their frequency by their relationship with the number of reviews divided by the number of reviews containing the word. This output is then normalised. Using a matrix of weightings like this should allow those terms that are less common but more indicative of a given class to have a greater influence on the resulting prediction accuracy. \n",
    "\n",
    "Rather than accepting the default parameters for the *TfidfVectorizer* function, I applied some variations which are likely to improve the predictive capacity when supplied to the SVM agorithm:\n",
    "- Set the *ngram_range* to (1,2), meaning that the vectoriser would capture both unigrams (as in Naive Bayes) but also bigrams (i.e. ordered words pairs). This would allow the matrix to record relationships between words and hopefully be better able to distinguish when certain combinations of words can be attributed to one class or another;\n",
    "- Used document frequency parameters of *min_df=3* and *max_df=0.8*. The minimum df setting means that words that appear in less than three reviews are excluded from the matrix counts, while the maximum df setting will conversly ignore words that are present in more than 80% of reveiws. The reasoning behind these settings are that if words are either far too common or extremely rare then they are unlikely to be of any significant use in predicting the class of any unseen reviews, and are thus better left out of the 'Bag of Words'. \n",
    "- The setting *sublinear_tf* was set to True, whereby the function will not assign linear weightings to term frequencies that appear at different rates. For example, a term that appears ten times is not ten times as likely to impact the review classifcation than one that appears only twice (Manning *et al.*, 2008). \n",
    "\n",
    "###### SVM Algorithm\n",
    "\n",
    "The SVM algorithm differs from the NB algoritm used in Task 1 in that it is not based on probabilities derived from the training data but rather on their spatial relationships and how they can be divided onto classes by a separating hyperplane (Russel and Norvig, 2020). This means that the relative proportions of postive and negative reviews supplied in the training data will not skew the expected probabilities towards one or another while also not needing to assume independance of data points for each token count or weight. \n",
    "\n",
    "Further, this function also handles high dimentionality very well, which is well suited to a very large matrix like that used here. Using a linear SVM, as I have in Task 2, gives the algorithm the ability to apply the training data to high-demensional space where the data is more easily seperated and a hyperplane applied when it would otherwise not be possible in a lower dimension space (Russel and Norvig, 2020). \n",
    "\n",
    "#### Implementation\n",
    "\n",
    "The implementation of the approach described above can be seen in the cell below, where the function *svm_sent_class* has been defined. Here I have applied the default values for the function as these align well with this task. Penalties, tolerances, and weights were all applied during the Tfidf vectorisation phase and therefore did not need to be applied again here. \n",
    "\n",
    "The split between training and testing data was done exactly as in Task 1, principally for ease of comparison. Also like Task 1, the output returns the predicted accuracy of the SVM model and a confusion matrix plot to show the number of correct and incorrect predictions.\n",
    "\n",
    "#### Comparison between task 1 & 2\n",
    "\n",
    "There are two main areas to compare these two algorithms - processing speed and prediction accuracy. On the first parameter they both run very quickly on this dataset and there is no significant difference between them, although it should be noted that the vectorisation for Task 2 was handling both unigrams and bigrams, giving it a larger processing load than that of Task 1. Given that the NB task ran very quickly there was no obvious need for efficiency gains in speed for Task 2, so only an increase in processing time would have seen a measurable decrease in our improvement estimation between the tasks.\n",
    "\n",
    "Task 1 returned a prediction accuracy of 79.42% on the testing data, while Task 2 had an accuracy of 83.75%, an increase of a little over 4%. This can be considered a measurable enough improvement between the tasks and therefore the changes to both the vectorisation process and the class classifier algorithm was somewhat successful. \n",
    "\n",
    "It is likely that this was partly due to the more accurate matrix that was created by the weighting vectorisation of the Tfidf function. If there were any pairs of words that were more indicative of one class over another then this change would have highlighted those, while the NB function would not. Furthermore, the inverse weighting of less common but more class-specific words likely also allowed for a better classification on the test data. The confusion matrix shows six more reviews for both negative and positive classes were correctly predicted using the Task 2 function over that of Task 1. \n",
    "\n",
    "Furthermore, while in this test data the class priors of the NB function were accurate for the given data balance and therefore did not result in a greater improvement in functionality in Task 2, the greater dimensionality of the data may have played a role in the improved scores due to the affinity for high dimension data of the SVM function. \n",
    "\n",
    "Ideally the best way to quantify which of these improvements was most responsible for the observed improvement would be to break the function down and note the incremental changes for each of the parameters. Furthermore, a k-fold analysis on a number of different random states would also return a more reliable value for the general predictive abilities of each function presented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_sent_class(df_in, test_perc=0.2, rand=99):\n",
    "    df = df_in.copy()\n",
    "    \n",
    "    # create vectoriser using the Tfidf module, creating unigrams and bigrams while ignoring very rare\n",
    "    # or very common words in each review\n",
    "    vec = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                          min_df=3,\n",
    "                          max_df=0.8,\n",
    "                          sublinear_tf=True)\n",
    "    \n",
    "    x = df['Reviews']\n",
    "    y = df['Sentiment']\n",
    "    \n",
    "    classes = ['Pos', 'Neg']\n",
    "    \n",
    "    # split the data into training and testing sets (80:20), stratified on the class to ensure an even\n",
    "    # spread of the two sentiment types\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, \n",
    "                                                        test_size=test_perc, random_state=rand)\n",
    "    \n",
    "    # transform the review data and transform it to a numerical array\n",
    "    x_train = vec.fit_transform(x_train).toarray()\n",
    "    x_test = vec.transform(x_test).toarray()\n",
    "    \n",
    "    # create the linear SVM model and fit the training data\n",
    "    model = svm.LinearSVC()\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # create a prediction on the test data and return the accuracy of the prediction, along with a \n",
    "    # confusion matrix plot of the results\n",
    "    prediction = model.predict(x_test)\n",
    "    accuracy = metrics.accuracy_score(prediction, y_test)\n",
    "    print(str('Predicted accuracy for Support Vector Machine is {:04.2f}'.format(accuracy * 100)) + '%')\n",
    "    print('\\n')\n",
    "    print(classification_report(y_test, prediction,target_names=classes))\n",
    "    con_mat = confusion_matrix(y_test, prediction)\n",
    "    figure = ConfusionMatrixDisplay(confusion_matrix=con_mat, display_labels=model.classes_)\n",
    "    figure = figure.plot(cmap=plt.cm.plasma)\n",
    "    figure.ax_.set(title='SVM Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted accuracy for Support Vector Machine is 83.75%\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pos       0.83      0.84      0.84       138\n",
      "         Neg       0.84      0.83      0.84       139\n",
      "\n",
      "    accuracy                           0.84       277\n",
      "   macro avg       0.84      0.84      0.84       277\n",
      "weighted avg       0.84      0.84      0.84       277\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "svm_sent_class(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### References\n",
    "\n",
    "- MANNING, C. D., RAGHAVAN P., and SCHUTZE, H., 2008. *Introduction to information retrieval*. Cambridge: Cambridge University Press.\n",
    "- RUSSELL, S. J., and NORVIG, P., 2020. *Artificial intelligence: a modern approach*. 4th Ed. Boston: Pearson.\n",
    "- STARMER, J., 2022. *The StatQuest illustrated guide to machine learning*. Great Britain: Amazon.\n",
    "- THOMAS, A., 2018. *The secret ratio that proves why customer reviews are so important* \\[Online\\]. Available from: https://www.inc.com/andrew-thomas/the-hidden-ratio-that-could-make-or-break-your-company.html \\[Accessed 8th August 2022\\]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
